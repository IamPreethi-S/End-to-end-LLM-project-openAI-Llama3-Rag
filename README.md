# End-to-end-LLM-project-openAI-Llama3-Ragpipeline

This project delivers an end-to-end LLM-based solution for domain-specific research and business understanding, built using LangChain for document processing and OpenAI embeddings for vectorization.
By implementing a Retrieval-Augmented Generation (RAG) pipeline, the system improves QA precision by 30%, enabling more relevant, accurate answers from internal documentation.


## Quick demo
- Attached Sustainability Report for video purpose
  
![Untitled video - Made with Clipchamp (1)](https://github.com/user-attachments/assets/067a910d-e00f-4f97-8991-ea48ffceb0ea)


## Overview

This project uses LangChain for text processing and OpenAI for vector conversion to create a robust domain analysis tool. The system processes documents, converts them into vectors, and uses a retrieval-augmented generation (RAG) pipeline for precise and relevant answers to user queries.


## Features

- **Text Processing**: Utilizes LangChain for efficient text processing.
- **Vector Conversion**: Employs OpenAI for converting text data into vectors.
- **Enhanced Search Relevance**: Increases search precision and relevance by 30%.
- **Retrieval QA**: Implements a retrieval-augmented generation (RAG) pipeline for accurate answers.
- **Document Processing**: Efficiently handles document loading, splitting, and retrieval.
- **Custom Chains and Prompts**: Allows the creation of custom processing chains and prompts for tailored use cases.


## Installation

To get started, clone the repository and install the required dependencies:


- git clone https://github.com/IamPreethi-S/End-to-end-LLM-project-using-OpenAI-Llama3-RAG.git

- cd End-to-end-LLM-project-using-OpenAI-Llama3-RAG

- pip install -r requirements.txt


## Future enhancements
- FAISS Product Quantization to compress embeddings reducing memory usage for vector DB


## Snapshots
![image](https://github.com/user-attachments/assets/a3472ba6-e8fd-4868-8501-72ac92b84b0b)



